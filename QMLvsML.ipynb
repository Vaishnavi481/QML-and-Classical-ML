{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlS8XAQCVYe1QJStVJ2oKr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaishnavi481/QML-and-Classical-ML/blob/main/QMLvsML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "6HzdBd9r_XYq"
      },
      "outputs": [],
      "source": [
        "# @title Install required libraries\n",
        "# Qiskit & Qiskit Machine Learning for quantum models\n",
        "# scikit-learn, pandas, numpy, matplotlib for classical ML and plotting\n",
        "\n",
        "!pip install -q qiskit qiskit-aer qiskit-machine-learning scikit-learn pandas numpy matplotlib\n",
        "!pip install -q qiskit-ibm-runtime qiskit-algorithms\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Imports, configuration, and result structure\n",
        "\n",
        "import time\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from qiskit import transpile\n",
        "\n",
        "\n",
        "# --- scikit-learn: datasets, preprocessing, models, metrics ---\n",
        "from sklearn.datasets import load_iris, load_breast_cancer, load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# --- Qiskit Machine Learning: quantum kernel machinery ---\n",
        "from qiskit.circuit.library import ZZFeatureMap\n",
        "from qiskit_machine_learning.kernels import FidelityQuantumKernel\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ExperimentResult:\n",
        "    \"\"\"\n",
        "    Container for one experiment run: one dataset, one model, one train_fraction.\n",
        "    This will be turned into rows of a Pandas DataFrame at the end.\n",
        "    \"\"\"\n",
        "    dataset: str\n",
        "    model: str\n",
        "    train_fraction: float\n",
        "\n",
        "    accuracy: float\n",
        "    f1_macro: float\n",
        "\n",
        "    train_time_sec: float\n",
        "    test_time_sec: float\n",
        "\n",
        "    # Quantum-specific fields (mostly None for classical models)\n",
        "    n_qubits: Optional[int] = None\n",
        "    n_shots: Optional[int] = None\n",
        "    n_train_samples: Optional[int] = None\n",
        "    n_test_samples: Optional[int] = None"
      ],
      "metadata": {
        "id": "hrMycJ-0_kmZ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q qiskit-ibm-runtime qiskit-algorithms"
      ],
      "metadata": {
        "id": "fiSWisWoLGZg"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initialize IBM Quantum account and Sampler for kernels\n",
        "\n",
        "from qiskit_ibm_runtime import QiskitRuntimeService, SamplerV2 as Sampler\n",
        "from qiskit_algorithms.state_fidelities import ComputeUncompute\n",
        "\n",
        "QiskitRuntimeService.save_account(\n",
        "     channel= \"ibm_quantum_platform\",\n",
        "     instance= \"crn:v1:bluemix:public:quantum-computing:us-east:a/00a4a61538784fb4902efee3054ee134:cf3f5ce5-3fea-4de7-a38a-dbf4fe160608::\",\n",
        "     token='0fdbqWmD6AnaCgHimNoU8_XyQ-KWNl-8lInS8TmqQLv2',\n",
        "     set_as_default=True,\n",
        "     overwrite=True,\n",
        ")\n",
        "service = QiskitRuntimeService()\n",
        "backend = service.least_busy(operational=True, simulator=False)\n",
        "backend.name\n",
        "\n",
        "# Build a SamplerV2 on this backend\n",
        "sampler_ibm = Sampler(mode=backend)\n",
        "\n",
        "# Build a ComputeUncompute fidelity object using this sampler\n",
        "fidelity_ibm = ComputeUncompute(sampler=sampler_ibm)\n"
      ],
      "metadata": {
        "id": "O0PhjVt2KAdM"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dataset loading utilities (Iris, Breast Cancer, Wine, Banknote)\n",
        "\n",
        "def load_iris_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Iris dataset\n",
        "    - 150 samples, 4 numeric features, 3 classes\n",
        "    \"\"\"\n",
        "    iris = load_iris()\n",
        "    X, y = iris.data, iris.target\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def load_breast_cancer_data() -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Breast Cancer Wisconsin (Diagnostic)\n",
        "    - 569 samples, 30 numeric features, binary labels (0/1)\n",
        "    \"\"\"\n",
        "    bc = load_breast_cancer()\n",
        "    X, y = bc.data, bc.target\n",
        "    return X, y\n",
        "\n",
        "    # Columns: variance, skewness, curtosis, entropy, class\n",
        "    X = df.iloc[:, :-1].values\n",
        "    y = df.iloc[:, -1].values\n",
        "    return X, y\n"
      ],
      "metadata": {
        "id": "676jS0H3E6Nl"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title QSVM using IBM Runtime backend (noisy or high-shot simulator)\n",
        "\n",
        "def run_quantum_kernel_svm_ibm(\n",
        "    dataset_name: str,\n",
        "    X_train: np.ndarray,\n",
        "    X_val: np.ndarray,\n",
        "    X_test: np.ndarray,\n",
        "    y_train: np.ndarray,\n",
        "    y_val: np.ndarray,\n",
        "    y_test: np.ndarray,\n",
        "    train_fraction: float,\n",
        "    max_features: int = 4,\n",
        "    C_values: List[float] = (0.1, 1.0, 10.0),\n",
        ") -> List[ExperimentResult]:\n",
        "    \"\"\"\n",
        "    Quantum kernel SVM, but kernel evaluations are done on an IBM Runtime backend\n",
        "    via a SamplerV2 + ComputeUncompute fidelity.\n",
        "\n",
        "    We also do a tiny grid search over C on the *validation* set to give QML a fair shot.\n",
        "    \"\"\"\n",
        "    results: List[ExperimentResult] = []\n",
        "\n",
        "    # Subsample train set\n",
        "    if train_fraction < 1.0:\n",
        "        X_train_full = X_train\n",
        "        y_train_full = y_train\n",
        "        X_train_sub, _, y_train_sub, _ = train_test_split(\n",
        "            X_train_full,\n",
        "            y_train_full,\n",
        "            train_size=train_fraction,\n",
        "            stratify=y_train_full,\n",
        "            random_state=RANDOM_STATE,\n",
        "        )\n",
        "    else:\n",
        "        X_train_sub, y_train_sub = X_train, y_train\n",
        "\n",
        "    # PCA + [0, π] scaling for quantum features\n",
        "    X_train_q, X_val_q, X_test_q, n_features_q = get_quantum_features(\n",
        "        X_train_sub,\n",
        "        X_val,\n",
        "        X_test,\n",
        "        max_features=max_features,\n",
        "    )\n",
        "\n",
        "    # Feature map + IBM-based fidelity kernel\n",
        "    feature_map = ZZFeatureMap(\n",
        "        feature_dimension=n_features_q,\n",
        "        reps=2,\n",
        "        entanglement=\"full\",\n",
        "    )\n",
        "\n",
        "    # FidelityQuantumKernel that uses the IBM sampler-based fidelity\n",
        "    qkernel_ibm = FidelityQuantumKernel(\n",
        "        feature_map=feature_map,\n",
        "        fidelity=fidelity_ibm,   # <-- uses your IBM backend\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"[QUANTUM-IBM] Dataset={dataset_name}, train_fraction={train_fraction}, \"\n",
        "        f\"n_qubits={n_features_q}, backend={backend.name}\"\n",
        "    )\n",
        "\n",
        "    # --- Tiny hyperparameter search on C, using validation set ---\n",
        "    best_C = None\n",
        "    best_val_acc = -np.inf\n",
        "    best_clf = None\n",
        "\n",
        "    for C in C_values:\n",
        "        print(f\"  - Trying C={C} ... (kernel matrices on IBM backend)\")\n",
        "\n",
        "        # Precompute kernel matrices (train/val)\n",
        "        K_train = qkernel_ibm.evaluate(x_vec=X_train_q)\n",
        "        K_val = qkernel_ibm.evaluate(x_vec=X_val_q, y_vec=X_train_q)\n",
        "\n",
        "        clf = SVC(kernel=\"precomputed\", C=C)\n",
        "\n",
        "        start_train = time.time()\n",
        "        clf.fit(K_train, y_train_sub)\n",
        "        end_train = time.time()\n",
        "\n",
        "        y_val_pred = clf.predict(K_val)\n",
        "        val_acc = accuracy_score(y_val, y_val_pred)\n",
        "\n",
        "        print(f\"    -> val_acc={val_acc:.3f}, train_time={end_train - start_train:.2f}s\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_C = C\n",
        "            best_clf = clf\n",
        "\n",
        "    # --- Evaluate on test set with best C ---\n",
        "    # Recompute kernels for best C (same quantum kernel, just new K_test)\n",
        "    K_train = qkernel_ibm.evaluate(x_vec=X_train_q)\n",
        "    K_test = qkernel_ibm.evaluate(x_vec=X_test_q, y_vec=X_train_q)\n",
        "\n",
        "    start_test = time.time()\n",
        "    y_test_pred = best_clf.predict(K_test)\n",
        "    end_test = time.time()\n",
        "\n",
        "    acc = accuracy_score(y_test, y_test_pred)\n",
        "    f1 = f1_score(y_test, y_test_pred, average=\"macro\")\n",
        "\n",
        "    print(\n",
        "        f\"  [IBM-QSVM result] C={best_C}, test_acc={acc:.3f}, \"\n",
        "        f\"test_f1={f1:.3f}, val_acc={best_val_acc:.3f}\"\n",
        "    )\n",
        "\n",
        "    # NOTE: kernel evaluation time lives inside fidelity / sampler; we do not split it cleanly.\n",
        "    # For reporting, you can measure wall-clock around evaluate() separately if you want.\n",
        "    results.append(\n",
        "        ExperimentResult(\n",
        "            dataset=dataset_name,\n",
        "            model=\"QSVM_FidelityKernel_IBM\",\n",
        "            train_fraction=train_fraction,\n",
        "            accuracy=acc,\n",
        "            f1_macro=f1,\n",
        "            train_time_sec=np.nan,   # optional: fill with measured time\n",
        "            test_time_sec=end_test - start_test,\n",
        "            n_qubits=n_features_q,\n",
        "            n_shots=None,            # you can set sampler_ibm.options.default_shots\n",
        "            n_train_samples=X_train_sub.shape[0],\n",
        "            n_test_samples=X_test.shape[0],\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "GOOt2mYzLf0m"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Train/val/test split and quantum feature preprocessing\n",
        "\n",
        "def train_val_test_split(\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    train_size: float = 0.6,\n",
        "    val_size: float = 0.2,\n",
        "    test_size: float = 0.2,\n",
        "    random_state: int = RANDOM_STATE,\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Split data into train / val / test for each dataset.\n",
        "    By default: 60% train, 20% val, 20% test (as in the plan).\n",
        "    \"\"\"\n",
        "    assert abs(train_size + val_size + test_size - 1.0) < 1e-8\n",
        "\n",
        "    # First split off the training set\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X,\n",
        "        y,\n",
        "        train_size=train_size,\n",
        "        stratify=y,\n",
        "        random_state=random_state,\n",
        "    )\n",
        "\n",
        "    # Then split the remaining into val and test\n",
        "    relative_val = val_size / (val_size + test_size)\n",
        "\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp,\n",
        "        y_temp,\n",
        "        train_size=relative_val,\n",
        "        stratify=y_temp,\n",
        "        random_state=random_state,\n",
        "    )\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "\n",
        "def get_quantum_features(\n",
        "    X_train: np.ndarray,\n",
        "    X_val: np.ndarray,\n",
        "    X_test: np.ndarray,\n",
        "    max_features: int = 4,\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, int]:\n",
        "    \"\"\"\n",
        "    Prepare features for quantum models.\n",
        "\n",
        "    Steps:\n",
        "    1. If number of features > max_features, reduce using PCA\n",
        "       (we keep max_features principal components).\n",
        "    2. Scale each feature into [0, π] for use as rotation angles\n",
        "       in the quantum feature map.\n",
        "    3. Return transformed train/val/test + the final feature dimension.\n",
        "\n",
        "    This function is intentionally separate so classical models\n",
        "    can still use the full feature space if desired.\n",
        "    \"\"\"\n",
        "    from sklearn.decomposition import PCA\n",
        "\n",
        "    n_features = X_train.shape[1]\n",
        "\n",
        "    # Optional PCA for dimensionality reduction\n",
        "    if n_features > max_features:\n",
        "        pca = PCA(n_components=max_features, random_state=RANDOM_STATE)\n",
        "        X_train_red = pca.fit_transform(X_train)\n",
        "        X_val_red = pca.transform(X_val)\n",
        "        X_test_red = pca.transform(X_test)\n",
        "        out_dim = max_features\n",
        "    else:\n",
        "        X_train_red = X_train\n",
        "        X_val_red = X_val\n",
        "        X_test_red = X_test\n",
        "        out_dim = n_features\n",
        "\n",
        "    # Scale features to [0, π] for rotation angles\n",
        "    scaler = MinMaxScaler(feature_range=(0.0, np.pi))\n",
        "    X_train_q = scaler.fit_transform(X_train_red)\n",
        "    X_val_q = scaler.transform(X_val_red)\n",
        "    X_test_q = scaler.transform(X_test_red)\n",
        "\n",
        "    return X_train_q, X_val_q, X_test_q, out_dim\n"
      ],
      "metadata": {
        "id": "v4kKm2CECk8F"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Classical model definitions and runner\n",
        "\n",
        "def build_classical_models() -> Dict[str, Pipeline]:\n",
        "    \"\"\"\n",
        "    Build the set of classical baseline models:\n",
        "      - Logistic Regression\n",
        "      - SVM with RBF kernel\n",
        "      - Random Forest\n",
        "      - Small MLP\n",
        "\n",
        "    Each model is wrapped in a Pipeline that applies StandardScaler\n",
        "    before the classifier.\n",
        "    \"\"\"\n",
        "    models = {\n",
        "        \"LogReg\": LogisticRegression(\n",
        "            max_iter=1000,\n",
        "            multi_class=\"auto\",\n",
        "            random_state=RANDOM_STATE\n",
        "        ),\n",
        "        \"SVM_RBF\": SVC(\n",
        "            kernel=\"rbf\",\n",
        "            probability=False,\n",
        "            random_state=RANDOM_STATE\n",
        "        ),\n",
        "        \"RandomForest\": RandomForestClassifier(\n",
        "            n_estimators=200,\n",
        "            random_state=RANDOM_STATE\n",
        "        ),\n",
        "        \"MLP\": MLPClassifier(\n",
        "            hidden_layer_sizes=(64, 32),\n",
        "            activation=\"relu\",\n",
        "            max_iter=500,\n",
        "            random_state=RANDOM_STATE\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    pipelines = {}\n",
        "    for name, clf in models.items():\n",
        "        pipelines[name] = Pipeline(\n",
        "            [\n",
        "                (\"scaler\", StandardScaler()),\n",
        "                (\"clf\", clf),\n",
        "            ]\n",
        "        )\n",
        "    return pipelines\n",
        "\n",
        "\n",
        "def run_classical_models(\n",
        "    dataset_name: str,\n",
        "    X_train: np.ndarray,\n",
        "    X_val: np.ndarray,\n",
        "    X_test: np.ndarray,\n",
        "    y_train: np.ndarray,\n",
        "    y_val: np.ndarray,\n",
        "    y_test: np.ndarray,\n",
        "    train_fraction: float,\n",
        ") -> List[ExperimentResult]:\n",
        "    \"\"\"\n",
        "    Run all classical baselines on a given dataset for a given train_fraction.\n",
        "\n",
        "    Steps:\n",
        "    - Optionally subsample the training set according to train_fraction.\n",
        "    - Train each classical model.\n",
        "    - Evaluate on the test set.\n",
        "    - Record accuracy, macro F1, and timing.\n",
        "\n",
        "    Returns a list of ExperimentResult objects.\n",
        "    \"\"\"\n",
        "    results: List[ExperimentResult] = []\n",
        "    models = build_classical_models()\n",
        "\n",
        "    # Subsample the training set for low-data regimes\n",
        "    if train_fraction < 1.0:\n",
        "        X_train_sub, _, y_train_sub, _ = train_test_split(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            train_size=train_fraction,\n",
        "            stratify=y_train,\n",
        "            random_state=RANDOM_STATE,\n",
        "        )\n",
        "    else:\n",
        "        X_train_sub, y_train_sub = X_train, y_train\n",
        "\n",
        "    for name, pipe in models.items():\n",
        "        print(f\"[CLASSICAL] Dataset={dataset_name}, Model={name}, train_fraction={train_fraction}\")\n",
        "\n",
        "        start_train = time.time()\n",
        "        pipe.fit(X_train_sub, y_train_sub)\n",
        "        end_train = time.time()\n",
        "\n",
        "        start_test = time.time()\n",
        "        y_pred = pipe.predict(X_test)\n",
        "        end_test = time.time()\n",
        "\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
        "\n",
        "        results.append(\n",
        "            ExperimentResult(\n",
        "                dataset=dataset_name,\n",
        "                model=name,\n",
        "                train_fraction=train_fraction,\n",
        "                accuracy=acc,\n",
        "                f1_macro=f1,\n",
        "                train_time_sec=end_train - start_train,\n",
        "                test_time_sec=end_test - start_test,\n",
        "                n_qubits=None,\n",
        "                n_shots=None,\n",
        "                n_train_samples=X_train_sub.shape[0],\n",
        "                n_test_samples=X_test.shape[0],\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "O6YLmjnLDHHz"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Quantum Kernel SVM (QSVM-style) runner\n",
        "\n",
        "def run_quantum_kernel_svm(\n",
        "    dataset_name: str,\n",
        "    X_train: np.ndarray,\n",
        "    X_val: np.ndarray,\n",
        "    X_test: np.ndarray,\n",
        "    y_train: np.ndarray,\n",
        "    y_val: np.ndarray,\n",
        "    y_test: np.ndarray,\n",
        "    train_fraction: float,\n",
        "    max_features: int = 4,\n",
        ") -> List[ExperimentResult]:\n",
        "    \"\"\"\n",
        "    Quantum kernel SVM experiment.\n",
        "\n",
        "    Steps:\n",
        "    - Optionally subsample training data (train_fraction).\n",
        "    - Apply PCA + scaling to [0, π] to get low-dimensional quantum features.\n",
        "    - Build a ZZFeatureMap with feature_dimension = number of quantum features.\n",
        "    - Create a FidelityQuantumKernel using this feature map.\n",
        "    - Train an SVC using the quantum kernel as a callable kernel.\n",
        "    - Evaluate on the test set.\n",
        "\n",
        "    Returns a list with a single ExperimentResult for QSVM.\n",
        "    \"\"\"\n",
        "    results: List[ExperimentResult] = []\n",
        "\n",
        "    # Subsample for low-data regimes\n",
        "    if train_fraction < 1.0:\n",
        "        X_train_full = X_train\n",
        "        y_train_full = y_train\n",
        "        X_train_sub, _, y_train_sub, _ = train_test_split(\n",
        "            X_train_full,\n",
        "            y_train_full,\n",
        "            train_size=train_fraction,\n",
        "            stratify=y_train_full,\n",
        "            random_state=RANDOM_STATE,\n",
        "        )\n",
        "    else:\n",
        "        X_train_sub, y_train_sub = X_train, y_train\n",
        "\n",
        "    # Quantum feature preprocessing (PCA + scaling)\n",
        "    X_train_q, X_val_q, X_test_q, n_features_q = get_quantum_features(\n",
        "        X_train_sub,\n",
        "        X_val,\n",
        "        X_test,\n",
        "        max_features=max_features,\n",
        "    )\n",
        "\n",
        "    # Feature map: encodes classical data into quantum states with entangling ZZ interactions\n",
        "    feature_map = ZZFeatureMap(\n",
        "        feature_dimension=n_features_q,\n",
        "        reps=2,\n",
        "        entanglement=\"full\",\n",
        "    )\n",
        "\n",
        "    # FidelityQuantumKernel uses an internal statevector or sampler backend by default\n",
        "    qkernel = FidelityQuantumKernel(feature_map=feature_map)\n",
        "\n",
        "    print(\n",
        "        f\"[QUANTUM] Dataset={dataset_name}, Model=QSVM_FidelityKernel, \"\n",
        "        f\"train_fraction={train_fraction}, n_qubits={n_features_q}\"\n",
        "    )\n",
        "\n",
        "    # SVC with callable kernel = quantum kernel evaluate function\n",
        "    qsvc = SVC(kernel=qkernel.evaluate)\n",
        "\n",
        "    start_train = time.time()\n",
        "    qsvc.fit(X_train_q, y_train_sub)\n",
        "    end_train = time.time()\n",
        "\n",
        "    start_test = time.time()\n",
        "    y_pred = qsvc.predict(X_test_q)\n",
        "    end_test = time.time()\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
        "\n",
        "    results.append(\n",
        "        ExperimentResult(\n",
        "            dataset=dataset_name,\n",
        "            model=\"QSVM_FidelityKernel\",\n",
        "            train_fraction=train_fraction,\n",
        "            accuracy=acc,\n",
        "            f1_macro=f1,\n",
        "            train_time_sec=end_train - start_train,\n",
        "            test_time_sec=end_test - start_test,\n",
        "            n_qubits=n_features_q,\n",
        "            n_shots=None,  # using analytic primitives; shots could be added with an explicit Sampler\n",
        "            n_train_samples=X_train_sub.shape[0],\n",
        "            n_test_samples=X_test.shape[0],\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "-HSsmMgdDHD7"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Quantum Kernel SVM (ideal, statevector-based) runner\n",
        "\n",
        "def run_quantum_kernel_svm(\n",
        "    dataset_name: str,\n",
        "    X_train: np.ndarray,\n",
        "    X_val: np.ndarray,\n",
        "    X_test: np.ndarray,\n",
        "    y_train: np.ndarray,\n",
        "    y_val: np.ndarray,\n",
        "    y_test: np.ndarray,\n",
        "    train_fraction: float,\n",
        "    max_features: int = 4,\n",
        ") -> List[ExperimentResult]:\n",
        "    \"\"\"\n",
        "    Quantum kernel SVM experiment (ideal, statevector-based).\n",
        "\n",
        "    - Subsample training set (train_fraction).\n",
        "    - PCA + scaling to [0, π] for 2–4 quantum features.\n",
        "    - ZZFeatureMap + FidelityQuantumKernel (default statevector fidelity).\n",
        "    - SVC with kernel=qkernel.evaluate.\n",
        "    \"\"\"\n",
        "    results: List[ExperimentResult] = []\n",
        "\n",
        "    # 1. Subsample for low-data regimes\n",
        "    if train_fraction < 1.0:\n",
        "        X_train_full = X_train\n",
        "        y_train_full = y_train\n",
        "        X_train_sub, _, y_train_sub, _ = train_test_split(\n",
        "            X_train_full,\n",
        "            y_train_full,\n",
        "            train_size=train_fraction,\n",
        "            stratify=y_train_full,\n",
        "            random_state=RANDOM_STATE,\n",
        "        )\n",
        "    else:\n",
        "        X_train_sub, y_train_sub = X_train, y_train\n",
        "\n",
        "    # 2. Quantum feature preprocessing (PCA + scaling to [0, π])\n",
        "    X_train_q, X_val_q, X_test_q, n_features_q = get_quantum_features(\n",
        "        X_train_sub,\n",
        "        X_val,\n",
        "        X_test,\n",
        "        max_features=max_features,\n",
        "    )\n",
        "\n",
        "    # 3. Feature map: encode data into quantum states\n",
        "    feature_map = ZZFeatureMap(\n",
        "        feature_dimension=n_features_q,\n",
        "        reps=2,\n",
        "        entanglement=\"full\",\n",
        "    )\n",
        "\n",
        "    # 4. FidelityQuantumKernel with default statevector fidelity\n",
        "    qkernel = FidelityQuantumKernel(feature_map=feature_map)\n",
        "\n",
        "    print(\n",
        "        f\"[QUANTUM] Dataset={dataset_name}, Model=QSVM_FidelityKernel, \"\n",
        "        f\"train_fraction={train_fraction}, n_qubits={n_features_q}\"\n",
        "    )\n",
        "\n",
        "    # 5. Train SVC with callable kernel\n",
        "    qsvc = SVC(kernel=qkernel.evaluate)\n",
        "\n",
        "    start_train = time.time()\n",
        "    qsvc.fit(X_train_q, y_train_sub)\n",
        "    end_train = time.time()\n",
        "\n",
        "    start_test = time.time()\n",
        "    y_pred = qsvc.predict(X_test_q)\n",
        "    end_test = time.time()\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
        "\n",
        "    results.append(\n",
        "        ExperimentResult(\n",
        "            dataset=dataset_name,\n",
        "            model=\"QSVM_FidelityKernel\",\n",
        "            train_fraction=train_fraction,\n",
        "            accuracy=acc,\n",
        "            f1_macro=f1,\n",
        "            train_time_sec=end_train - start_train,\n",
        "            test_time_sec=end_test - start_test,\n",
        "            n_qubits=n_features_q,\n",
        "            n_shots=None,\n",
        "            n_train_samples=X_train_sub.shape[0],\n",
        "            n_test_samples=X_test.shape[0],\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "MoxoF3CRNJBr"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run all experiments across datasets and training fractions\n",
        "\n",
        "def run_all_experiments(\n",
        "    output_csv: str = \"results_qml_vs_classical.csv\",\n",
        "    train_fractions: Optional[List[float]] = None,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    High-level experiment driver.\n",
        "\n",
        "    For each dataset:\n",
        "      - Load data\n",
        "      - Split into train/val/test\n",
        "      - For each train_fraction:\n",
        "          * Run classical baselines\n",
        "          * Run quantum kernel SVM\n",
        "    Save all results into a CSV and also return as a DataFrame.\n",
        "    \"\"\"\n",
        "    if train_fractions is None:\n",
        "        # Fractions of the training set to actually use\n",
        "        train_fractions = [0.1, 0.3, 0.6, 1.0]\n",
        "\n",
        "    # Map dataset names to loader functions\n",
        "    datasets_config = {\n",
        "        \"iris\": load_iris_data,\n",
        "        \"breast_cancer\": load_breast_cancer_data,\n",
        "        \"banknote\": load_banknote_data,\n",
        "        \"wine\": load_wine_data,\n",
        "    }\n",
        "\n",
        "    all_results: List[ExperimentResult] = []\n",
        "\n",
        "    for ds_name, loader in datasets_config.items():\n",
        "        print(f\"\\n================ Dataset: {ds_name} ================\")\n",
        "        X, y = loader()\n",
        "\n",
        "        # Same split for all models\n",
        "        X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(X, y)\n",
        "\n",
        "        for frac in train_fractions:\n",
        "            # Run classical models\n",
        "            classical_results = run_classical_models(\n",
        "                dataset_name=ds_name,\n",
        "                X_train=X_train,\n",
        "                X_val=X_val,\n",
        "                X_test=X_test,\n",
        "                y_train=y_train,\n",
        "                y_val=y_val,\n",
        "                y_test=y_test,\n",
        "                train_fraction=frac,\n",
        "            )\n",
        "            all_results.extend(classical_results)\n",
        "\n",
        "            # Run quantum kernel SVM\n",
        "            quantum_results = run_quantum_kernel_svm(\n",
        "                dataset_name=ds_name,\n",
        "                X_train=X_train,\n",
        "                X_val=X_val,\n",
        "                X_test=X_test,\n",
        "                y_train=y_train,\n",
        "                y_val=y_val,\n",
        "                y_test=y_test,\n",
        "                train_fraction=frac,\n",
        "                max_features=4,  # 2–4 qubits as per plan\n",
        "            )\n",
        "            all_results.extend(quantum_results)\n",
        "\n",
        "    # Convert to DataFrame and save\n",
        "    df = pd.DataFrame([asdict(r) for r in all_results])\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    print(f\"\\n[INFO] Saved all results to {output_csv}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# Actually run everything\n",
        "df_results = run_all_experiments()\n",
        "df_results.head()\n",
        "# @title Quick sanity check and overall summary\n",
        "\n",
        "# Take a peek at the first few rows\n",
        "display(df_results.head())\n",
        "\n",
        "print(\"\\nModels:\", sorted(df_results[\"model\"].unique()))\n",
        "print(\"Datasets:\", sorted(df_results[\"dataset\"].unique()))\n",
        "print(\"Train fractions:\", sorted(df_results[\"train_fraction\"].unique()))\n",
        "\n",
        "# Overall mean accuracy / F1 by model (across datasets & fractions)\n",
        "summary_overall = (\n",
        "    df_results.groupby(\"model\")[[\"accuracy\", \"f1_macro\"]]\n",
        "    .mean()\n",
        "    .sort_values(\"accuracy\", ascending=False)\n",
        "    .round(3)\n",
        ")\n",
        "print(\"\\nOverall mean performance by model:\")\n",
        "display(summary_overall)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4V1TQg9DG_D",
        "outputId": "05cd4971-3688-44f7-f355-52c82df27c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================ Dataset: iris ================\n",
            "[CLASSICAL] Dataset=iris, Model=LogReg, train_fraction=0.1\n",
            "[CLASSICAL] Dataset=iris, Model=SVM_RBF, train_fraction=0.1\n",
            "[CLASSICAL] Dataset=iris, Model=RandomForest, train_fraction=0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLASSICAL] Dataset=iris, Model=MLP, train_fraction=0.1\n",
            "[QUANTUM] Dataset=iris, Model=QSVM_FidelityKernel, train_fraction=0.1, n_qubits=4\n",
            "[CLASSICAL] Dataset=iris, Model=LogReg, train_fraction=0.3\n",
            "[CLASSICAL] Dataset=iris, Model=SVM_RBF, train_fraction=0.3\n",
            "[CLASSICAL] Dataset=iris, Model=RandomForest, train_fraction=0.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLASSICAL] Dataset=iris, Model=MLP, train_fraction=0.3\n",
            "[QUANTUM] Dataset=iris, Model=QSVM_FidelityKernel, train_fraction=0.3, n_qubits=4\n",
            "[CLASSICAL] Dataset=iris, Model=LogReg, train_fraction=0.6\n",
            "[CLASSICAL] Dataset=iris, Model=SVM_RBF, train_fraction=0.6\n",
            "[CLASSICAL] Dataset=iris, Model=RandomForest, train_fraction=0.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLASSICAL] Dataset=iris, Model=MLP, train_fraction=0.6\n",
            "[QUANTUM] Dataset=iris, Model=QSVM_FidelityKernel, train_fraction=0.6, n_qubits=4\n",
            "[CLASSICAL] Dataset=iris, Model=LogReg, train_fraction=1.0\n",
            "[CLASSICAL] Dataset=iris, Model=SVM_RBF, train_fraction=1.0\n",
            "[CLASSICAL] Dataset=iris, Model=RandomForest, train_fraction=1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLASSICAL] Dataset=iris, Model=MLP, train_fraction=1.0\n",
            "[QUANTUM] Dataset=iris, Model=QSVM_FidelityKernel, train_fraction=1.0, n_qubits=4\n",
            "\n",
            "================ Dataset: breast_cancer ================\n",
            "[CLASSICAL] Dataset=breast_cancer, Model=LogReg, train_fraction=0.1\n",
            "[CLASSICAL] Dataset=breast_cancer, Model=SVM_RBF, train_fraction=0.1\n",
            "[CLASSICAL] Dataset=breast_cancer, Model=RandomForest, train_fraction=0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLASSICAL] Dataset=breast_cancer, Model=MLP, train_fraction=0.1\n",
            "[QUANTUM] Dataset=breast_cancer, Model=QSVM_FidelityKernel, train_fraction=0.1, n_qubits=4\n",
            "[CLASSICAL] Dataset=breast_cancer, Model=LogReg, train_fraction=0.3\n",
            "[CLASSICAL] Dataset=breast_cancer, Model=SVM_RBF, train_fraction=0.3\n",
            "[CLASSICAL] Dataset=breast_cancer, Model=RandomForest, train_fraction=0.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLASSICAL] Dataset=breast_cancer, Model=MLP, train_fraction=0.3\n",
            "[QUANTUM] Dataset=breast_cancer, Model=QSVM_FidelityKernel, train_fraction=0.3, n_qubits=4\n",
            "[CLASSICAL] Dataset=breast_cancer, Model=LogReg, train_fraction=0.6\n",
            "[CLASSICAL] Dataset=breast_cancer, Model=SVM_RBF, train_fraction=0.6\n",
            "[CLASSICAL] Dataset=breast_cancer, Model=RandomForest, train_fraction=0.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLASSICAL] Dataset=breast_cancer, Model=MLP, train_fraction=0.6\n",
            "[QUANTUM] Dataset=breast_cancer, Model=QSVM_FidelityKernel, train_fraction=0.6, n_qubits=4\n",
            "[CLASSICAL] Dataset=breast_cancer, Model=LogReg, train_fraction=1.0\n",
            "[CLASSICAL] Dataset=breast_cancer, Model=SVM_RBF, train_fraction=1.0\n",
            "[CLASSICAL] Dataset=breast_cancer, Model=RandomForest, train_fraction=1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLASSICAL] Dataset=breast_cancer, Model=MLP, train_fraction=1.0\n",
            "[QUANTUM] Dataset=breast_cancer, Model=QSVM_FidelityKernel, train_fraction=1.0, n_qubits=4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Main results table (train_fraction = 1.0) and LaTeX export\n",
        "\n",
        "# Filter to full-training runs\n",
        "df_full = df_results[df_results[\"train_fraction\"] == 1.0].copy()\n",
        "\n",
        "# Average over multiple random splits if you ever run more than once\n",
        "table_full = (\n",
        "    df_full\n",
        "    .groupby([\"dataset\", \"model\"])[[\"accuracy\", \"f1_macro\"]]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Round for nicer display\n",
        "table_full[\"accuracy\"] = table_full[\"accuracy\"].round(3)\n",
        "table_full[\"f1_macro\"] = table_full[\"f1_macro\"].round(3)\n",
        "\n",
        "print(\"Main results (full training data):\")\n",
        "display(table_full)\n"
      ],
      "metadata": {
        "id": "E-KQ_zb0ZHh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title LaTeX table: dataset × model (full training data)\n",
        "\n",
        "latex_full = table_full.to_latex(\n",
        "    index=False,\n",
        "    caption=\"Classification accuracy and macro-F1 at full training data for all datasets and models.\",\n",
        "    label=\"tab:full_results\",\n",
        "    float_format=\"%.3f\",\n",
        ")\n",
        "print(latex_full)\n"
      ],
      "metadata": {
        "id": "5fQnhgwiZJIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Wide LaTeX table: models as rows, datasets as columns (accuracy)\n",
        "\n",
        "# Pivot on accuracy\n",
        "acc_pivot = (\n",
        "    df_full\n",
        "    .groupby([\"model\", \"dataset\"])[\"accuracy\"]\n",
        "    .mean()\n",
        "    .unstack(\"dataset\")\n",
        "    .round(3)\n",
        ")\n",
        "\n",
        "print(\"Accuracy (full training) per model and dataset:\")\n",
        "display(acc_pivot)\n",
        "\n",
        "latex_acc = acc_pivot.to_latex(\n",
        "    caption=\"Test accuracy at full training fraction, for each model and dataset.\",\n",
        "    label=\"tab:accuracy_full_wide\",\n",
        "    float_format=\"%.3f\",\n",
        ")\n",
        "print(latex_acc)\n"
      ],
      "metadata": {
        "id": "ppBtth-nZKqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Quick accuracy plots: accuracy vs train_fraction per dataset\n",
        "\n",
        "def plot_accuracy_vs_fraction(df: pd.DataFrame, dataset_name: str):\n",
        "    \"\"\"\n",
        "    For a given dataset, plot accuracy vs train_fraction for each model.\n",
        "    \"\"\"\n",
        "    sub = df[df[\"dataset\"] == dataset_name]\n",
        "    models = sorted(sub[\"model\"].unique())\n",
        "\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    for m in models:\n",
        "        mdata = sub[sub[\"model\"] == m].sort_values(\"train_fraction\")\n",
        "        plt.plot(\n",
        "            mdata[\"train_fraction\"],\n",
        "            mdata[\"accuracy\"],\n",
        "            marker=\"o\",\n",
        "            label=m,\n",
        "        )\n",
        "\n",
        "    plt.title(f\"Accuracy vs Train Fraction ({dataset_name})\")\n",
        "    plt.xlabel(\"Train fraction of training set\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "for ds in sorted(df_results[\"dataset\"].unique()):\n",
        "    plot_accuracy_vs_fraction(df_results, ds)\n"
      ],
      "metadata": {
        "id": "IAs2LFAcDG6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Contrast table: accuracy at 10% vs 100% train fraction\n",
        "\n",
        "low_frac = 0.1\n",
        "high_frac = 1.0\n",
        "\n",
        "df_low = df_results[df_results[\"train_fraction\"] == low_frac]\n",
        "df_high = df_results[df_results[\"train_fraction\"] == high_frac]\n",
        "\n",
        "# Mean across any internal randomness\n",
        "df_low_mean = (\n",
        "    df_low.groupby([\"dataset\", \"model\"])[[\"accuracy\", \"f1_macro\"]]\n",
        "    .mean()\n",
        "    .rename(columns={\"accuracy\": \"acc_low\", \"f1_macro\": \"f1_low\"})\n",
        ")\n",
        "df_high_mean = (\n",
        "    df_high.groupby([\"dataset\", \"model\"])[[\"accuracy\", \"f1_macro\"]]\n",
        "    .mean()\n",
        "    .rename(columns={\"accuracy\": \"acc_high\", \"f1_high\": \"f1_high\"})\n",
        ")\n",
        "\n",
        "contrast = (\n",
        "    df_low_mean\n",
        "    .join(df_high_mean)\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "for col in [\"acc_low\", \"acc_high\", \"f1_low\", \"f1_high\"]:\n",
        "    contrast[col] = contrast[col].round(3)\n",
        "\n",
        "print(f\"Accuracy/F1 at {int(low_frac*100)}% vs {int(high_frac*100)}% of training data:\")\n",
        "display(contrast)\n"
      ],
      "metadata": {
        "id": "r2vmqVosZMdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot accuracy and macro-F1 vs train fraction (per dataset) and save as PDF\n",
        "\n",
        "def plot_acc_f1_vs_fraction(df: pd.DataFrame, dataset_name: str, save_pdf: bool = True):\n",
        "    \"\"\"\n",
        "    Generates a 2-row figure for one dataset:\n",
        "      - Top: accuracy vs train_fraction for each model\n",
        "      - Bottom: macro F1 vs train_fraction for each model\n",
        "\n",
        "    If save_pdf=True, saves as '{dataset_name}_acc_f1_vs_fraction.pdf'.\n",
        "    \"\"\"\n",
        "    sub = df[df[\"dataset\"] == dataset_name].copy()\n",
        "    models = sorted(sub[\"model\"].unique())\n",
        "\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(7, 6), sharex=True)\n",
        "\n",
        "    # Accuracy subplot\n",
        "    ax_acc = axes[0]\n",
        "    for m in models:\n",
        "        mdata = (\n",
        "            sub[sub[\"model\"] == m]\n",
        "            .sort_values(\"train_fraction\")\n",
        "        )\n",
        "        ax_acc.plot(\n",
        "            mdata[\"train_fraction\"],\n",
        "            mdata[\"accuracy\"],\n",
        "            marker=\"o\",\n",
        "            linestyle=\"-\",\n",
        "            label=m,\n",
        "        )\n",
        "    ax_acc.set_ylabel(\"Accuracy\")\n",
        "    ax_acc.set_title(f\"{dataset_name}: Accuracy vs training fraction\")\n",
        "    ax_acc.grid(True)\n",
        "    ax_acc.legend(loc=\"best\")\n",
        "\n",
        "    # F1 subplot\n",
        "    ax_f1 = axes[1]\n",
        "    for m in models:\n",
        "        mdata = (\n",
        "            sub[sub[\"model\"] == m]\n",
        "            .sort_values(\"train_fraction\")\n",
        "        )\n",
        "        ax_f1.plot(\n",
        "            mdata[\"train_fraction\"],\n",
        "            mdata[\"f1_macro\"],\n",
        "            marker=\"o\",\n",
        "            linestyle=\"-\",\n",
        "            label=m,\n",
        "        )\n",
        "    ax_f1.set_xlabel(\"Train fraction of training split\")\n",
        "    ax_f1.set_ylabel(\"Macro F1\")\n",
        "    ax_f1.set_title(f\"{dataset_name}: Macro F1 vs training fraction\")\n",
        "    ax_f1.grid(True)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    if save_pdf:\n",
        "        filename = f\"{dataset_name}_acc_f1_vs_fraction.pdf\"\n",
        "        fig.savefig(filename, bbox_inches=\"tight\")\n",
        "        print(f\"[INFO] Saved figure to {filename}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "for ds in sorted(df_results[\"dataset\"].unique()):\n",
        "    plot_acc_f1_vs_fraction(df_results, ds, save_pdf=True)\n"
      ],
      "metadata": {
        "id": "M0mjiZOIZPXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot training time vs train fraction (per dataset)\n",
        "\n",
        "def plot_train_time_vs_fraction(df: pd.DataFrame, dataset_name: str, save_pdf: bool = False):\n",
        "    sub = df[df[\"dataset\"] == dataset_name].copy()\n",
        "    models = sorted(sub[\"model\"].unique())\n",
        "\n",
        "    plt.figure(figsize=(7, 4))\n",
        "    for m in models:\n",
        "        mdata = (\n",
        "            sub[sub[\"model\"] == m]\n",
        "            .sort_values(\"train_fraction\")\n",
        "        )\n",
        "        plt.plot(\n",
        "            mdata[\"train_fraction\"],\n",
        "            mdata[\"train_time_sec\"],\n",
        "            marker=\"o\",\n",
        "            linestyle=\"-\",\n",
        "            label=m,\n",
        "        )\n",
        "\n",
        "    plt.title(f\"{dataset_name}: Training time vs training fraction\")\n",
        "    plt.xlabel(\"Train fraction of training split\")\n",
        "    plt.ylabel(\"Training time (seconds)\")\n",
        "    plt.grid(True)\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_pdf:\n",
        "        filename = f\"{dataset_name}_train_time_vs_fraction.pdf\"\n",
        "        plt.savefig(filename, bbox_inches=\"tight\")\n",
        "        print(f\"[INFO] Saved figure to {filename}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "for ds in sorted(df_results[\"dataset\"].unique()):\n",
        "    plot_train_time_vs_fraction(df_results, ds, save_pdf=False)\n"
      ],
      "metadata": {
        "id": "LPnXL7-bZRTT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}